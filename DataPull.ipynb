{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import mysql.connector\n",
    "from pyspark.sql import SQLContext\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Basics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydb = mysql.connector.connect(host=\"localhost\",user=\"root\",passwd=\"\",database=\"staging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycursor = mydb.cursor(buffered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropCreateTable(schema, tablename):\n",
    "    try:\n",
    "        mycursor.execute(\"DROP TABLE IF EXISTS \"+tablename)\n",
    "        mycursor.execute(\"CREATE TABLE \" + tablename + schema)\n",
    "        mydb.commit\n",
    "    except:\n",
    "        mydb.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populateData(df, country, tablename):\n",
    "    try:\n",
    "        y = spark.sql(\"\"\"select * from {1} where country = '{0}'\"\"\".format(country, df))\n",
    "        y.write.format(\"jdbc\")\\\n",
    "        .option(\"url\", \"jdbc:mysql://localhost/staging\")\\\n",
    "        .option(\"driver\", \"com.mysql.jdbc.Driver\")\\\n",
    "        .option(\"dbtable\", tablename)\\\n",
    "        .option(\"user\", \"root\")\\\n",
    "        .option(\"truncate\", \"true\")\\\n",
    "        .option(\"password\", \"\")\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .save()\n",
    "    except Exception as e: \n",
    "        print(\"Error in the data added!!!\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.options(delimiter = '|', header = True).csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----------+---------+-------------------+--------------+-------+-----+-------+--------+---------+\n",
      "|  H|Customer_Name|Customer_Id|Open_Date|Last_Consulted_Date|Vaccination_Id|Dr_Name|State|Country|     DOB|Is_Active|\n",
      "+---+-------------+-----------+---------+-------------------+--------------+-------+-----+-------+--------+---------+\n",
      "|  D|         Alex|     123457| 20101012|           20121013|           MVD|   Paul|   SA|    USA|06031987|        A|\n",
      "|  D|         John|     123458| 20101012|           20121013|           MVD|   Paul|   TN|    IND|06031987|        A|\n",
      "|  D|       Mathew|     123459| 20101012|           20121013|           MVD|   Paul|  WAS|   PHIL|06031987|        A|\n",
      "|  D|         Matt|      12345| 20101012|           20121013|           MVD|   Paul|  BOS|    NYC|06031987|        A|\n",
      "|  D|        Jacob|       1256| 20101012|           20121013|           MVD|   Paul|  VIC|     AU|06031987|        A|\n",
      "|  D|        Alex1|     123457| 20101012|           20121013|           MVD|   Paul|   SA|    USA|06031987|        A|\n",
      "|  D|        John1|     123458| 20101012|           20121013|           MVD|   Paul|   TN|    IND|06031987|        A|\n",
      "|  D|      Mathew1|     123459| 20101012|           20121013|           MVD|   Paul|  WAS|   PHIL|06031987|        A|\n",
      "|  D|        Matt1|      12345| 20101012|           20121013|           MVD|   Paul|  BOS|    NYC|06031987|        A|\n",
      "|  D|       Jacob1|       1256| 20101012|           20121013|           MVD|   Paul|  VIC|     AU|06031987|        A|\n",
      "|  D|        Alex2|     123457| 20101012|           20121013|           MVD|   Paul|   SA|    USA|06031987|        A|\n",
      "|  D|        John2|     123458| 20101012|           20121013|           MVD|   Paul|   TN|    IND|06031987|        A|\n",
      "|  D|      Mathew2|     123459| 20101012|           20121013|           MVD|   Paul|  WAS|   PHIL|06031987|        A|\n",
      "|  D|        Matt2|      12345| 20101012|           20121013|           MVD|   Paul|  BOS|    NYC|06031987|        A|\n",
      "|  D|       Jacob2|       1256| 20101012|           20121013|           MVD|   Paul|  VIC|     AU|06031987|        A|\n",
      "|  D|        Alex3|     123457| 20101012|           20121013|           MVD|   Paul|   SA|    USA|06031987|        A|\n",
      "|  D|        John3|     123458| 20101012|           20121013|           MVD|   Paul|   TN|    IND|06031987|        A|\n",
      "|  D|      Mathew3|     123459| 20101012|           20121013|           MVD|   Paul|  WAS|   PHIL|06031987|        A|\n",
      "|  D|        Matt3|      12345| 20101012|           20121013|           MVD|   Paul|  BOS|    NYC|06031987|        A|\n",
      "|  D|       Jacob3|       1256| 20101012|           20121013|           MVD|   Paul|  VIC|     AU|06031987|        A|\n",
      "+---+-------------+-----------+---------+-------------------+--------------+-------+-----+-------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('datainput')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = spark.sql('SELECT DISTINCT country FROM datainput')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|country|\n",
      "+-------+\n",
      "|     AU|\n",
      "|    USA|\n",
      "|   PHIL|\n",
      "|    IND|\n",
      "|    NYC|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"\"\"(\n",
    "CustomerName VARCHAR(255) PRIMARY KEY,\n",
    "CustomerID VARCHAR(18) NOT NULL,\n",
    "CustomerOpenDate DATE NOT NULL,\n",
    "LastConsultedDate DATE,\n",
    "VaccinationType CHAR(5),\n",
    "DoctorConsulted  CHAR(255),\n",
    "State CHAR(5),\n",
    "Country CHAR(5),\n",
    "PostCode INT(5),\n",
    "DateofBirth  DATE,\n",
    "ActiveCustomer CHAR(1)\n",
    ");\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = spark.sql(\"\"\"SELECT Customer_Name AS CustomerName\n",
    ", Customer_Id AS CustomerId\n",
    ", CONCAT_WS('-',SUBSTRING(Open_Date, 1, 4),SUBSTRING(Open_Date, 5, 2),SUBSTRING(Open_Date, 7, 2)) AS CustomerOpenDate\n",
    ", CONCAT_WS('-',SUBSTRING(Last_Consulted_Date, 1, 4),SUBSTRING(Last_Consulted_Date, 5, 2),SUBSTRING(Last_Consulted_Date, 7, 2)) AS LastConsultedDate\n",
    ", Vaccination_Id AS VaccinationType \n",
    ", Dr_Name AS DoctorConsulted \n",
    ", State\n",
    ", Country\n",
    ", 1 AS PostCode\n",
    ", CONCAT_WS('-',SUBSTRING(DOB, 5, 4),SUBSTRING(DOB, 3, 2),SUBSTRING(DOB, 1, 2)) AS DateofBirth\n",
    ", Is_Active AS ActiveCustomer \n",
    "FROM datainput\n",
    "WHERE Customer_Id IS NOT NULL AND Open_Date IS NOT NULL\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.createOrReplaceTempView('proc_data_sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = country.rdd.toLocalIterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tab in iterator:\n",
    "    tablename = \"staging.customer_vaccination_\" + tab[\"country\"]\n",
    "    dropCreateTable(schema, tablename)\n",
    "    populateData(\"proc_data_sql\", tab[\"country\"], tablename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
